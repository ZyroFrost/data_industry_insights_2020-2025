import psycopg2
import psycopg2.extras
import pandas as pd
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
import os
from typing import List, Dict
from datetime import datetime

load_dotenv()

BASE_DIR = Path(__file__).resolve().parents[2]
DATA_DIR = BASE_DIR / "data" / "data_processed"

DB_CONFIG = {
    "host": os.getenv("DB_SUPABASE_HOST"),
    "port": os.getenv("DB_SUPABASE_PORT"),
    "dbname": os.getenv("DB_SUPABASE_NAME"),
    "user": os.getenv("DB_SUPABASE_USER"),
    "password": os.getenv("DB_SUPABASE_PASS"),
}

BATCH_SIZE = 5000

TABLE_MAPPINGS = {
    "companies.csv": {
        "table": "companies",
        "id_column": "company_id",
        "columns": ["company_name", "size", "industry"]
    },
    "locations.csv": {
        "table": "locations",
        "id_column": "location_id",
        "columns": ["city", "country", "country_iso", "latitude", "longitude", "population"]
    },
    "role_names.csv": {
        "table": "role_names",
        "id_column": "role_id",
        "columns": ["role_name"]
    },
    "skills.csv": {
        "table": "skills",
        "id_column": "skill_id",
        "columns": ["skill_name", "skill_category"]
    },
    "job_postings.csv": {
        "table": "job_postings",
        "id_column": "job_id",
        "columns": ["company_id", "location_id", "posted_date", "min_salary", 
                   "max_salary", "currency", "required_exp_years", "education_level", 
                   "employment_type", "remote_option", "job_description"]
    },
    "job_roles.csv": {
        "table": "job_roles",
        "id_column": None,
        "columns": ["job_id", "role_id"]
    },
    "job_levels.csv": {
        "table": "job_levels",
        "id_column": None,
        "columns": ["job_id", "level"]
    },
    "job_skills.csv": {
        "table": "job_skills",
        "id_column": None,
        "columns": ["job_id", "skill_id"]
    }
}

ID_MAPPING = {
    "company_id": {},
    "location_id": {},
    "role_id": {},
    "skill_id": {},
    "job_id": {}
}

TABLE_STATS = {
    "companies": {"inserted": 0, "skipped": 0},
    "locations": {"inserted": 0, "skipped": 0},
    "role_names": {"inserted": 0, "skipped": 0},
    "skills": {"inserted": 0, "skipped": 0},
    "job_postings": {"inserted": 0, "skipped": 0},
    "job_roles": {"inserted": 0, "skipped": 0},
    "job_levels": {"inserted": 0, "skipped": 0},
    "job_skills": {"inserted": 0, "skipped": 0},
}

def log_progress(message, prefix="‚ÑπÔ∏è"):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {prefix} {message}", flush=True)

def clean_value(value, column_name=""):
    if pd.isna(value):
        return None
    
    if isinstance(value, str):
        stripped = value.strip()
        if stripped in ["__NA__", "__INVALID__", "__UNMATCHED__", '_NA_', 'NA', 'nan', 'NaN', '']:
            return None
        if column_name == "population":
            try:
                return int(float(stripped))
            except:
                return None
        return stripped

    if isinstance(value, (np.integer, np.floating)):
        if np.isnan(value):
            return None
        if isinstance(value, np.integer):
            return int(value)
        float_val = float(value)
        if column_name in ['population', 'required_exp_years']:
            return int(float_val)
        return float_val
    
    if isinstance(value, (bool, np.bool_)):
        return bool(value)
    
    return value

def get_db_connection():
    try:
        log_progress("ƒêang k·∫øt n·ªëi database...")
        conn = psycopg2.connect(**DB_CONFIG, connect_timeout=30)
        conn.autocommit = False
        cur = conn.cursor()
        conn.commit()
        cur.close()
        log_progress("K·∫øt n·ªëi database th√†nh c√¥ng", "‚úì")
        return conn
    except Exception as e:
        log_progress(f"L·ªói k·∫øt n·ªëi: {e}", "‚ùå")
        return None

def get_table_row_count(conn, table_name: str) -> int:
    """ƒê·∫øm s·ªë rows hi·ªán t·∫°i trong b·∫£ng"""
    cursor = conn.cursor()
    try:
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count = cursor.fetchone()[0]
        cursor.close()
        return count
    except Exception as e:
        cursor.close()
        return 0

def check_all_tables_status(conn) -> Dict[str, int]:
    """Check status c·ªßa t·∫•t c·∫£ c√°c b·∫£ng"""
    log_progress("ƒêang ki·ªÉm tra database hi·ªán t·∫°i...")
    
    all_tables = [
        "companies", "locations", "role_names", "skills",
        "job_postings", "job_roles", "job_levels", "job_skills"
    ]
    
    table_counts = {}
    for table in all_tables:
        count = get_table_row_count(conn, table)
        table_counts[table] = count
    
    return table_counts

def load_existing_id_mapping(conn, id_column: str, table_name: str):
    """Load ID mapping t·ª´ DB (n·∫øu b·∫£ng ƒë√£ c√≥ data v√† kh√¥ng x√≥a)"""
    cursor = conn.cursor()
    try:
        cursor.execute(f"SELECT {id_column} FROM {table_name}")
        rows = cursor.fetchall()
        cursor.close()
        
        for row in rows:
            new_id = row[0]
            ID_MAPPING[id_column][new_id] = new_id
        
        log_progress(f"ƒê√£ load {len(rows):,} {id_column} mapping t·ª´ DB", "‚úì")
    except Exception as e:
        cursor.close()
        log_progress(f"Kh√¥ng th·ªÉ load mapping: {e}", "‚ö†Ô∏è")

def batch_insert_with_returning(conn, table_name, columns, data_batch, id_column):
    """Batch insert v·ªõi RETURNING"""
    if not data_batch:
        return []
    
    cols_str = ", ".join(columns)
    placeholders = ", ".join(["%s"] * len(columns))
    
    values_list = []
    flat_values = []
    for row in data_batch:
        values_list.append(f"({placeholders})")
        flat_values.extend(row)
    
    query = f"INSERT INTO {table_name} ({cols_str}) VALUES {', '.join(values_list)} RETURNING {id_column}"
    
    cursor = conn.cursor()
    try:
        cursor.execute(query, flat_values)
        new_ids = [row[0] for row in cursor.fetchall()]
        conn.commit()
        cursor.close()
        return new_ids
    except Exception as e:
        conn.rollback()
        cursor.close()
        raise e

def batch_insert_no_returning(conn, table_name, columns, data_batch):
    """Batch insert kh√¥ng RETURNING"""
    if not data_batch:
        return
    
    cols_str = ", ".join(columns)
    cursor = conn.cursor()
    try:
        query = f"INSERT INTO {table_name} ({cols_str}) VALUES %s"
        psycopg2.extras.execute_values(cursor, query, data_batch, page_size=BATCH_SIZE)
        conn.commit()
        cursor.close()
    except Exception as e:
        conn.rollback()
        cursor.close()
        raise e

def validate_fk_exists(fk_value, fk_column, allow_null=False):
    """Ki·ªÉm tra FK c√≥ t·ªìn t·∫°i kh√¥ng"""
    if fk_value is None:
        return allow_null
    
    if fk_column in ID_MAPPING:
        return fk_value in ID_MAPPING[fk_column]
    
    return True

def get_csv_row_count(file_path: Path) -> int:
    """ƒê·∫øm s·ªë rows trong CSV file"""
    try:
        df = pd.read_csv(file_path, low_memory=False, on_bad_lines='skip')
        return len(df)
    except Exception as e:
        log_progress(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c {file_path.name}: {e}", "‚ö†Ô∏è")
        return 0

def load_csv_to_db_optimized(conn, csv_file: str, table_name: str, id_column: str, 
                             columns: List[str], skip: bool = False):
    """Load CSV v·ªõi option truncate"""
    if skip:
        log_progress(f"B·ªè qua {csv_file} (marked as skip)", "‚è≠Ô∏è")
        return True
    
    file_path = DATA_DIR / csv_file
    if not file_path.exists():
        log_progress(f"File kh√¥ng t·ªìn t·∫°i: {csv_file}", "‚ö†Ô∏è")
        return False
    
    # Check DB hi·ªán t·∫°i
    current_count = get_table_row_count(conn, table_name)
    csv_count = get_csv_row_count(file_path)
    
    # So s√°nh v√† hi·ªÉn th·ªã
    print(f"\nüìã B·∫£ng: {table_name}")
    print(f"   DB hi·ªán t·∫°i: {current_count:,}/{csv_count:,} rows", end="")
    
    if current_count == 0:
        print(" üÜï (tr·ªëng)")
    elif current_count < csv_count:
        missing = csv_count - current_count
        print(f" ‚ö†Ô∏è  (thi·∫øu {missing:,} rows)")
    elif current_count == csv_count:
        print(" ‚úì (ƒë·∫ßy ƒë·ªß)")
    elif current_count > csv_count:
        extra = current_count - csv_count
        print(f" ‚ùì (th·ª´a {extra:,} rows)")
    
    choice = input(f"   X√≥a v√† load l·∫°i? (y/n): ").lower().strip()
    
    if choice != 'y':
        log_progress(f"Skip {table_name} - gi·ªØ nguy√™n data hi·ªán t·∫°i", "‚è≠Ô∏è")
        
        # Load ID mapping t·ª´ DB ƒë·ªÉ FK v·∫´n ho·∫°t ƒë·ªông
        if id_column and current_count > 0:
            load_existing_id_mapping(conn, id_column, table_name)
        
        return True
    
    # Truncate b·∫£ng
    log_progress(f"X√≥a d·ªØ li·ªáu b·∫£ng {table_name}...")
    cursor = conn.cursor()
    try:
        cursor.execute(f"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE;")
        conn.commit()
        cursor.close()
        log_progress(f"ƒê√£ x√≥a {table_name}", "‚úì")
    except Exception as e:
        conn.rollback()
        cursor.close()
        log_progress(f"L·ªói truncate: {e}", "‚ùå")
        return False
    
    # Ti·∫øp t·ª•c load CSV
    file_size_mb = file_path.stat().st_size / (1024 * 1024)
    log_progress(f"ƒê·ªçc {csv_file} ({file_size_mb:.1f}MB)...")
    
    try:
        df = pd.read_csv(file_path, low_memory=False, on_bad_lines='skip')
        df.columns = df.columns.str.strip()
        total_rows = len(df)
        
        log_progress(f"ƒê√£ ƒë·ªçc {total_rows:,} rows, b·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
        
        if df.empty:
            return True
        
        data_batch = []
        old_ids = []
        inserted_count = 0
        skipped_count = 0
        last_pct = 0
        
        for idx, row in df.iterrows():
            pct = int((idx / total_rows) * 100)
            if pct >= last_pct + 10:
                log_progress(f"  {pct}% ({idx:,}/{total_rows:,}) | inserted: {inserted_count:,} | skipped: {skipped_count:,}")
                last_pct = pct
            
            try:
                old_id = int(row[id_column]) if id_column and id_column in df.columns else None
                values = [clean_value(row[col], col) for col in columns]
                
                # Validation
                if table_name == "companies" and values[0] is None:
                    skipped_count += 1
                    continue
                
                if table_name == "role_names" and values[0] is None:
                    skipped_count += 1
                    continue
                
                if table_name == "job_postings":
                    for i, col in enumerate(columns):
                        if col in ["min_salary", "max_salary", "required_exp_years", "posted_date"]:
                            if values[i] in ["__NA__", "__INVALID__", "__UNMATCHED__"]:
                                values[i] = None
                    
                    company_id = values[0]
                    location_id = values[1]
                    
                    if company_id is None or location_id is None:
                        skipped_count += 1
                        continue
                    
                    if not validate_fk_exists(company_id, "company_id"):
                        if skipped_count < 10:
                            log_progress(f"Row {idx}: company_id={company_id} kh√¥ng t·ªìn t·∫°i", "‚ö†Ô∏è")
                        skipped_count += 1
                        continue
                    
                    if not validate_fk_exists(location_id, "location_id"):
                        if skipped_count < 10:
                            log_progress(f"Row {idx}: location_id={location_id} kh√¥ng t·ªìn t·∫°i", "‚ö†Ô∏è")
                        skipped_count += 1
                        continue
                    
                    if len(values) > 8 and values[8] not in ["Full-time", "Part-time", "Internship", "Temporary", None]:
                        values[8] = None
                    
                    if len(values) > 9 and values[9] in [None]:
                        values[9] = None
                
                if table_name == "companies":
                    if values[1] not in ["Startup", "Small", "Medium", "Large", "Enterprise", None]:
                        values[1] = None
                    if values[2] not in ["Technology", "Finance", "Banking", "Insurance",
                        "Healthcare", "Education", "E-commerce", "Manufacturing",
                        "Consulting", "Government", "Telecommunications", "Energy",
                        "Retail", "Logistics", "Real Estate", None]:
                        values[2] = None
                
                data_batch.append(values)
                if old_id:
                    old_ids.append(old_id)
                
                # Insert batch
                if len(data_batch) >= BATCH_SIZE:
                    try:
                        if id_column:
                            new_ids = batch_insert_with_returning(conn, table_name, columns, data_batch, id_column)
                            for old, new in zip(old_ids, new_ids):
                                ID_MAPPING[id_column][old] = new
                        else:
                            batch_insert_no_returning(conn, table_name, columns, data_batch)
                        
                        inserted_count += len(data_batch)
                        data_batch = []
                        old_ids = []
                    except Exception as batch_error:
                        log_progress(f"Batch l·ªói t·∫°i row ~{idx}: {str(batch_error)[:100]}", "‚ùå")
                        skipped_count += len(data_batch)
                        data_batch = []
                        old_ids = []
                        continue
                
            except Exception as e:
                skipped_count += 1
                if skipped_count <= 5:
                    log_progress(f"Row {idx}: {str(e)[:80]}", "‚ö†Ô∏è")
                continue
        
        # Insert batch cu·ªëi
        if data_batch:
            try:
                if id_column:
                    new_ids = batch_insert_with_returning(conn, table_name, columns, data_batch, id_column)
                    for old, new in zip(old_ids, new_ids):
                        ID_MAPPING[id_column][old] = new
                else:
                    batch_insert_no_returning(conn, table_name, columns, data_batch)
                inserted_count += len(data_batch)
            except Exception as batch_error:
                log_progress(f"Batch cu·ªëi l·ªói: {str(batch_error)[:100]}", "‚ùå")
                skipped_count += len(data_batch)
        
        TABLE_STATS[table_name]["inserted"] = inserted_count
        TABLE_STATS[table_name]["skipped"] = skipped_count
        
        log_progress(f"{table_name}: {inserted_count:,} inserted, {skipped_count:,} skipped", "‚úì")
        return True
        
    except Exception as e:
        conn.rollback()
        log_progress(f"L·ªói: {e}", "‚ùå")
        import traceback
        traceback.print_exc()
        return False

def load_csv_with_fk_mapping_optimized(conn, csv_file: str, table_name: str, 
                                      columns: List[str], skip: bool = False):
    """Load junction tables"""
    if skip:
        log_progress(f"B·ªè qua {csv_file} (marked as skip)", "‚è≠Ô∏è")
        return True
    
    file_path = DATA_DIR / csv_file
    if not file_path.exists():
        log_progress(f"File kh√¥ng t·ªìn t·∫°i: {csv_file}", "‚ö†Ô∏è")
        return False
    
    # Check DB hi·ªán t·∫°i
    current_count = get_table_row_count(conn, table_name)
    csv_count = get_csv_row_count(file_path)
    
    # So s√°nh v√† hi·ªÉn th·ªã
    print(f"\nüìã B·∫£ng: {table_name}")
    print(f"   DB hi·ªán t·∫°i: {current_count:,}/{csv_count:,} rows", end="")
    
    if current_count == 0:
        print(" üÜï (tr·ªëng)")
    elif current_count < csv_count:
        missing = csv_count - current_count
        print(f" ‚ö†Ô∏è  (thi·∫øu {missing:,} rows)")
    elif current_count == csv_count:
        print(" ‚úì (ƒë·∫ßy ƒë·ªß)")
    elif current_count > csv_count:
        extra = current_count - csv_count
        print(f" ‚ùì (th·ª´a {extra:,} rows)")
    
    choice = input(f"   X√≥a v√† load l·∫°i? (y/n): ").lower().strip()
    
    if choice != 'y':
        log_progress(f"Skip {table_name} - gi·ªØ nguy√™n data hi·ªán t·∫°i", "‚è≠Ô∏è")
        return True
    
    # Truncate
    log_progress(f"X√≥a d·ªØ li·ªáu b·∫£ng {table_name}...")
    cursor = conn.cursor()
    try:
        cursor.execute(f"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE;")
        conn.commit()
        cursor.close()
        log_progress(f"ƒê√£ x√≥a {table_name}", "‚úì")
    except Exception as e:
        conn.rollback()
        cursor.close()
        log_progress(f"L·ªói truncate: {e}", "‚ùå")
        return False
    
    # Ti·∫øp t·ª•c load CSV
    file_size_mb = file_path.stat().st_size / (1024 * 1024)
    log_progress(f"ƒê·ªçc {csv_file} ({file_size_mb:.1f}MB)...")
    
    try:
        df = pd.read_csv(file_path, low_memory=False, on_bad_lines='skip')
        df.columns = df.columns.str.strip()
        total_rows = len(df)
        
        log_progress(f"ƒê√£ ƒë·ªçc {total_rows:,} rows, b·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
        
        if df.empty:
            return True
        
        data_batch = []
        inserted_count = 0
        skipped_count = 0
        last_pct = 0
        
        for idx, row in df.iterrows():
            pct = int((idx / total_rows) * 100)
            if pct >= last_pct + 10:
                log_progress(f"  {pct}% ({idx:,}/{total_rows:,}) | inserted: {inserted_count:,} | skipped: {skipped_count:,}")
                last_pct = pct
            
            try:
                values = []
                for col in columns:
                    val = clean_value(row[col], col)
                    if col == "level" and val is None:
                        val = "__NA__"
                    
                    if col == "job_id":
                        values.append(val)
                        continue
                    
                    if col in ID_MAPPING and val is not None:
                        if val not in ID_MAPPING[col]:
                            raise ValueError(f"FK {col}={val} kh√¥ng t·ªìn t·∫°i")
                        val = ID_MAPPING[col][val]
                    
                    values.append(val)
                
                if any(v is None for v in values[:2]):
                    skipped_count += 1
                    continue
                
                data_batch.append(values)
                
                if len(data_batch) >= BATCH_SIZE:
                    try:
                        batch_insert_no_returning(conn, table_name, columns, data_batch)
                        inserted_count += len(data_batch)
                        data_batch = []
                    except Exception as batch_error:
                        log_progress(f"Batch l·ªói t·∫°i row ~{idx}: {str(batch_error)[:100]}", "‚ùå")
                        skipped_count += len(data_batch)
                        data_batch = []
                        continue
                
            except Exception as e:
                skipped_count += 1
                if skipped_count <= 5:
                    log_progress(f"Row {idx}: {str(e)[:80]}", "‚ö†Ô∏è")
                continue
        
        if data_batch:
            try:
                batch_insert_no_returning(conn, table_name, columns, data_batch)
                inserted_count += len(data_batch)
            except Exception as batch_error:
                log_progress(f"Batch cu·ªëi l·ªói: {str(batch_error)[:100]}", "‚ùå")
                skipped_count += len(data_batch)
        
        TABLE_STATS[table_name]["inserted"] = inserted_count
        TABLE_STATS[table_name]["skipped"] = skipped_count
        
        log_progress(f"{table_name}: {inserted_count:,} inserted, {skipped_count:,} skipped", "‚úì")
        return True
        
    except Exception as e:
        conn.rollback()
        log_progress(f"L·ªói: {e}", "‚ùå")
        return False

def main():
    print("=" * 70)
    print("üöÄ UPLOAD D·ªÆ LI·ªÜU (SMART MODE)")
    print("=" * 70)
    
    conn = get_db_connection()
    if not conn:
        return
    
    log_progress(f"Data directory: {DATA_DIR}")
    log_progress(f"Batch size: {BATCH_SIZE:,} rows/batch")
    
    # Check t·∫•t c·∫£ b·∫£ng tr∆∞·ªõc
    table_counts = check_all_tables_status(conn)
    
    print("\n" + "=" * 70)
    print("üìä DATABASE HI·ªÜN T·∫†I")
    print("=" * 70)
    for table, count in table_counts.items():
        print(f"{table:<20} | {count:>10,} rows")
    
    print("\n" + "=" * 70)
    print("üí° B·∫°n s·∫Ω ƒë∆∞·ª£c h·ªèi t·ª´ng b·∫£ng tr∆∞·ªõc khi load")
    print("   Y = X√≥a v√† load l·∫°i | N = Gi·ªØ nguy√™n data c≈©")
    print("=" * 70)
    
    input("\nNh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c...")
    
    start_time = datetime.now()
    success_count = 0
    
    # PHASE 1
    print("\n" + "=" * 70)
    print("üì¶ PHASE 1: Parent tables")
    print("=" * 70)
    parent_tables = ["companies.csv", "locations.csv", "role_names.csv", "skills.csv"]
    for csv_file in parent_tables:
        if csv_file in TABLE_MAPPINGS:
            config = TABLE_MAPPINGS[csv_file]
            if load_csv_to_db_optimized(conn, csv_file, config["table"], 
                             config["id_column"], config["columns"], 
                             config.get("skip", False)):
                success_count += 1
    
    # PHASE 2
    print("\n" + "=" * 70)
    print("üì¶ PHASE 2: Job postings")
    print("=" * 70)
    config = TABLE_MAPPINGS["job_postings.csv"]
    if load_csv_to_db_optimized(conn, "job_postings.csv", config["table"], 
                     config["id_column"], config["columns"], 
                     config.get("skip", False)):
        success_count += 1
    
    # PHASE 3
    print("\n" + "=" * 70)
    print("üì¶ PHASE 3: Junction tables")
    print("=" * 70)
    junction_tables = ["job_roles.csv", "job_levels.csv", "job_skills.csv"]
    for csv_file in junction_tables:
        if csv_file in TABLE_MAPPINGS:
            config = TABLE_MAPPINGS[csv_file]
            if load_csv_with_fk_mapping_optimized(conn, csv_file, config["table"], 
                                       config["columns"], config.get("skip", False)):
                success_count += 1
    
    conn.close()
    
    elapsed = datetime.now() - start_time
    
    print("\n" + "=" * 70)
    print(f"‚úÖ HO√ÄN TH√ÄNH")
    print(f"‚è±Ô∏è  T·ªïng th·ªùi gian: {elapsed}")
    print("=" * 70)
    
    print("\nüìä LOAD SUMMARY")
    print("-" * 70)
    for table, stat in TABLE_STATS.items():
        if stat['inserted'] > 0 or stat['skipped'] > 0:
            print(f"{table:<20} | inserted: {stat['inserted']:>10,} | skipped: {stat['skipped']:>8,}")
    
    print("\nüìä ID MAPPING")
    print("-" * 70)
    for id_col, mapping in ID_MAPPING.items():
        if mapping:
            print(f"{id_col:<20} | {len(mapping):>10,} mappings")
    
    total_inserted = sum(s['inserted'] for s in TABLE_STATS.values())
    if elapsed.total_seconds() > 0:
        rows_per_sec = total_inserted / elapsed.total_seconds()
        print(f"\n‚ö° Throughput: {rows_per_sec:,.0f} rows/second")

if __name__ == "__main__":
    main()